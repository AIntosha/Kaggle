{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 1. Import libs\n",
    "# 2. procs \\ cat_names \\ cont_names \\ dep_var\n",
    "# 3. data\n",
    "# 4. Learn\n",
    "# 5. first fits\n",
    "# 6. lr_find \\ plot\n",
    "# 7. Save model before tuning\n",
    "# 8. unfreeze / fit\n",
    "\n",
    "# lr find def\n",
    "# lr find auto\n",
    "# lr find india"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import libs\n",
    "\n",
    "from fastai.tabular import *\n",
    "from fastai.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. procs \\ cat_names \\ cont_names \\ dep_var \n",
    "\n",
    "# transformations\n",
    "procs = [FillMissing, Categorify, Normalize]\n",
    "\n",
    "# words\n",
    "cat_names = []\n",
    "\n",
    "# numbers\n",
    "cont_names = ['SepalLengthCm','SepalWidthCm','PetalLengthCm', 'PetalWidthCm']\n",
    "\n",
    "# target\n",
    "dep_var = 'Species'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. data\n",
    "\n",
    "data = (TabularList.from_df(df, procs=procs, cont_names=cont_names)\n",
    "        .split_subsets(train_size=0.6,valid_size=0.4,seed=42)\n",
    "        .label_from_df(cols=dep_var)\n",
    "        .databunch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Learn\n",
    "\n",
    "learn = tabular_learner(data, layers=[200,100], metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. first fits\n",
    "\n",
    "learn.fit_one_cycle(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. lr_find \\ plot\n",
    "\n",
    "learn.lr_find();\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save model before tuning\n",
    "\n",
    "learn.save('stage-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. unfreeze / fit\n",
    "\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr find def\n",
    "\n",
    "learn.unfreeze()\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = learn.recorder.min_grad_lr\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr find auto\n",
    "\n",
    "def find_appropriate_lr(model:Learner, lr_diff:int = 15, loss_threshold:float = .05, adjust_value:float = 1, plot:bool = False) -> float:\n",
    "    #Run the Learning Rate Finder\n",
    "    model.lr_find()\n",
    "    \n",
    "    #Get loss values and their corresponding gradients, and get lr values\n",
    "    losses = np.array(model.recorder.losses)\n",
    "    assert(lr_diff < len(losses))\n",
    "    loss_grad = np.gradient(losses)\n",
    "    lrs = model.recorder.lrs\n",
    "    \n",
    "    #Search for index in gradients where loss is lowest before the loss spike\n",
    "    #Initialize right and left idx using the lr_diff as a spacing unit\n",
    "    #Set the local min lr as -1 to signify if threshold is too low\n",
    "    r_idx = -1\n",
    "    l_idx = r_idx - lr_diff\n",
    "    while (l_idx >= -len(losses)) and (abs(loss_grad[r_idx] - loss_grad[l_idx]) > loss_threshold):\n",
    "        local_min_lr = lrs[l_idx]\n",
    "        r_idx -= 1\n",
    "        l_idx -= 1\n",
    "\n",
    "    lr_to_use = local_min_lr * adjust_value\n",
    "    \n",
    "    if plot:\n",
    "        # plots the gradients of the losses in respect to the learning rate change\n",
    "        plt.plot(loss_grad)\n",
    "        plt.plot(len(losses)+l_idx, loss_grad[l_idx],markersize=10,marker='o',color='red')\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"Index of LRs\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(np.log10(lrs), losses)\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"Log 10 Transform of Learning Rate\")\n",
    "        loss_coord = np.interp(np.log10(lr_to_use), np.log10(lrs), losses)\n",
    "        plt.plot(np.log10(lr_to_use), loss_coord, markersize=10,marker='o',color='red')\n",
    "        plt.show()\n",
    "        \n",
    "    return lr_to_use\n",
    "find_appropriate_lr(model=learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr find india\n",
    "\n",
    "def find_lr(losses, lrs):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    losses_skipped = 5\n",
    "    trailing_losses_skipped = 5\n",
    "    losses = losses[losses_skipped:-trailing_losses_skipped]\n",
    "    lrs = lrs[losses_skipped:-trailing_losses_skipped]\n",
    "\n",
    "    n = len(losses)\n",
    "\n",
    "    max_start = 0\n",
    "    max_end = 0\n",
    "    \n",
    "    # finding the longest valley.\n",
    "    lds = [1] * n\n",
    "\n",
    "    for i in range(1, n):\n",
    "        for j in range(0, i):\n",
    "            if losses[i] < losses[j] and lds[i] < lds[j] + 1:\n",
    "                lds[i] = lds[j] + 1\n",
    "            if lds[max_end] < lds[i]:\n",
    "                max_end = i\n",
    "                max_start = max_end - lds[max_end]\n",
    "\n",
    "    sections = (max_end - max_start) / 3\n",
    "    final_index = max_start + int(sections) + int(sections/2) # pick something midway, or 2/3rd of the way to be more aggressive\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.plot(\n",
    "        lrs,\n",
    "        losses\n",
    "    )\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_xlabel(\"Learning Rate\")\n",
    "    ax.set_xscale('log')\n",
    "    ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))\n",
    "    ax.plot(\n",
    "        lrs[final_index],\n",
    "        losses[final_index],\n",
    "        markersize=10,\n",
    "        marker='o',\n",
    "        color='red'\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return lrs[final_index]\n",
    "find_lr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
