{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to default Classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import libraries\n",
    "# 2. X, Y, shape\n",
    "# 3. test_split if needed\n",
    "\n",
    "### LogisticRegression\n",
    "# 4. logreg = LogisticRegression()\n",
    "# 5. logreg.fit(X, Y)\n",
    "# 6. Y_pred = logreg.predict(X)\n",
    "# 7. acc_logreg = logreg.score(X, Y)\n",
    "\n",
    "### NaiveBayes\n",
    "# 8. naivebayes = GaussianNB()\n",
    "# 9. naivebayes.fit(X, Y)\n",
    "# 10. Y_pred = naivebayes.predict(X)\n",
    "# 11. acc_naivebayes = naivebayes.score(X, Y)\n",
    "\n",
    "### SVM\n",
    "# 12. svm = SVC()\n",
    "# 13. svm.fit(X, Y)\n",
    "# 14. Y_pred = svm.predict(X)\n",
    "# 15. acc_svm = svm.score(X, Y)\n",
    "\n",
    "### AdaBoost\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# adaBoost = AdaBoostClassifier(base_estimator=None, learning_rate=1.0, n_estimators=100)\n",
    "# adaBoost.fit(X, Y)\n",
    "# Y_pred = adaBoost.predict(X)\n",
    "# acc_ada = adaBoost.score(X, Y)\n",
    "# print(acc_ada)\n",
    "\n",
    "\n",
    "### XGBoost\n",
    "# import xgboost as xgb\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# parameters = {\n",
    "#     'max_depth': [3, 4, 5, 6, 7, 8], \n",
    "#     'n_estimators': [5, 10, 20, 50, 100],\n",
    "#     'learning_rate': np.linspace(0.02,0.16,8)\n",
    "# }\n",
    "\n",
    "# xgb_model = GridSearchCV(xgb.XGBClassifier(), parameters, cv=5).fit(X, Y)\n",
    "\n",
    "# print(accuracy_score(Y, xgb_model.predict(X)))\n",
    "# print(xgb_model.best_score_)\n",
    "# print(xgb_model.best_params_)\n",
    "# print(xgb_model.best_estimator_)\n",
    "\n",
    "# acc_xgb = round(xgb_model.score(X, Y) * 100, 2)\n",
    "# print(acc_xgb)\n",
    "\n",
    "\n",
    "### CatBoost\n",
    "# import catboost as cb\n",
    "\n",
    "# parameters = {'iterations': [10, 50, 100],\n",
    "#               'learning_rate': np.linspace(0.02,0.16,4),\n",
    "#               'depth': range(4,10)\n",
    "# }\n",
    "\n",
    "# catb_model = GridSearchCV(cb.CatBoostClassifier(verbose=False), parameters, cv=5).fit(X, Y)\n",
    "\n",
    "# print(accuracy_score(Y, catb_model.predict(X)))\n",
    "# print(catb_model.best_score_)\n",
    "# print(catb_model.best_params_)\n",
    "\n",
    "# acc_catb = round(catb_model.score(X, Y) * 100, 2)\n",
    "# print(acc_catb)\n",
    "\n",
    "\n",
    "### LightGBM\n",
    "# import lightgbm as lgbm\n",
    "\n",
    "# parameters = {'n_estimators': [5, 50, 100],\n",
    "#               'learning_rate': np.linspace(0.02,0.16,4),\n",
    "#               'num_leaves': [31, 61],\n",
    "#               'min_data_in_leaf': [20, 30, 40],\n",
    "#               'max_depth': range(3,8),\n",
    "# }\n",
    "\n",
    "# lgbm_model = GridSearchCV(lgbm.LGBMClassifier(), parameters, cv=5).fit(X, Y)\n",
    "\n",
    "# print(accuracy_score(Y, lgbm_model.predict(X)))\n",
    "# print(lgbm_model.best_score_)\n",
    "# print(lgbm_model.best_params_)\n",
    "# print(lgbm_model.best_estimator_)\n",
    "\n",
    "# acc_lgbm = round(lgbm_model.score(X, Y) * 100, 2)\n",
    "# print(acc_lgbm)\n",
    "\n",
    "\n",
    "\n",
    "## universal code\n",
    "## universal code #2\n",
    "# experimenting with different n values in KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import libraries\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. X, Y, shape\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "Y = df['target']\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. test_split if needed\n",
    "\n",
    "x_train, y_train, x_test, y_test = train_test_split(X, Y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# universal cell for classification\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "naive_bayes = GaussianNB()\n",
    "svc = SVC()\n",
    "linear_svc = LinearSVC()\n",
    "knn = KNeighborsClassifier()\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "perceptron = Perceptron()\n",
    "sgd = SGDClassifier()\n",
    "random_forest =  RandomForestClassifier()\n",
    "\n",
    "logreg.fit(X, Y)\n",
    "Y_pred = logreg.predict(X)\n",
    "acc_logreg = logreg.score(X, Y)\n",
    "\n",
    "naive_bayes.fit(X, Y)\n",
    "Y_pred = naive_bayes.predict(X)\n",
    "acc_naive_bayes = naive_bayes.score(X, Y)\n",
    "\n",
    "svc.fit(X, Y)\n",
    "Y_pred = svc.predict(X)\n",
    "acc_svc = svc.score(X, Y)\n",
    "\n",
    "linear_svc.fit(X, Y)\n",
    "Y_pred = linear_svc.predict(X)\n",
    "acc_linear_svc = linear_svc.score(X, Y)\n",
    "\n",
    "knn.fit(X, Y)\n",
    "Y_pred = knn.predict(X)\n",
    "acc_knn = knn.score(X, Y)\n",
    "\n",
    "dec_tree.fit(X, Y)\n",
    "Y_pred = dec_tree.predict(X)\n",
    "acc_dec_tree = dec_tree.score(X, Y)\n",
    "\n",
    "perceptron.fit(X, Y)\n",
    "Y_pred = perceptron.predict(X)\n",
    "acc_perceptron = perceptron.score(X, Y)\n",
    "\n",
    "sgd.fit(X, Y)\n",
    "Y_pred = sgd.predict(X)\n",
    "acc_sgd = sgd.score(X, Y)\n",
    "\n",
    "random_forest.fit(X, Y)\n",
    "Y_pred = random_forest.predict(X)\n",
    "acc_random_forest = random_forest.score(X, Y)\n",
    "\n",
    "\n",
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "              'Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "              'Stochastic Gradient Decent', 'Linear SVC', \n",
    "              'Decision Tree'],\n",
    "    'Score': [acc_svc, acc_knn, acc_logreg, \n",
    "              acc_random_forest, acc_naive_bayes, acc_perceptron, \n",
    "              acc_sgd, acc_linear_svc, acc_dec_tree]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# universal cell for classification #2\n",
    "\n",
    "def universal():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.svm import SVC, LinearSVC\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.linear_model import Perceptron\n",
    "    from sklearn.linear_model import SGDClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    naive_bayes = GaussianNB()\n",
    "    svc = SVC()\n",
    "    linear_svc = LinearSVC()\n",
    "    knn = KNeighborsClassifier()\n",
    "    dec_tree = DecisionTreeClassifier()\n",
    "    perceptron = Perceptron()\n",
    "    sgd = SGDClassifier()\n",
    "    random_forest =  RandomForestClassifier()\n",
    "\n",
    "    logreg.fit(X, Y)\n",
    "    Y_pred = logreg.predict(X)\n",
    "    acc_logreg = logreg.score(X, Y)\n",
    "\n",
    "    naive_bayes.fit(X, Y)\n",
    "    Y_pred = naive_bayes.predict(X)\n",
    "    acc_naive_bayes = naive_bayes.score(X, Y)\n",
    "\n",
    "    svc.fit(X, Y)\n",
    "    Y_pred = svc.predict(X)\n",
    "    acc_svc = svc.score(X, Y)\n",
    "\n",
    "    linear_svc.fit(X, Y)\n",
    "    Y_pred = linear_svc.predict(X)\n",
    "    acc_linear_svc = linear_svc.score(X, Y)\n",
    "\n",
    "    knn.fit(X, Y)\n",
    "    Y_pred = knn.predict(X)\n",
    "    acc_knn = knn.score(X, Y)\n",
    "\n",
    "    dec_tree.fit(X, Y)\n",
    "    Y_pred = dec_tree.predict(X)\n",
    "    acc_dec_tree = dec_tree.score(X, Y)\n",
    "\n",
    "    perceptron.fit(X, Y)\n",
    "    Y_pred = perceptron.predict(X)\n",
    "    acc_perceptron = perceptron.score(X, Y)\n",
    "\n",
    "    sgd.fit(X, Y)\n",
    "    Y_pred = sgd.predict(X)\n",
    "    acc_sgd = sgd.score(X, Y)\n",
    "\n",
    "    random_forest.fit(X, Y)\n",
    "    Y_pred = random_forest.predict(X)\n",
    "    acc_random_forest = random_forest.score(X, Y)\n",
    "\n",
    "\n",
    "    models = pd.DataFrame({\n",
    "        'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "                  'Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "                  'Stochastic Gradient Decent', 'Linear SVC', \n",
    "                  'Decision Tree'],\n",
    "        'Score': [acc_svc, acc_knn, acc_logreg, \n",
    "                  acc_random_forest, acc_naive_bayes, acc_perceptron, \n",
    "                  acc_sgd, acc_linear_svc, acc_dec_tree]})\n",
    "    print(models.sort_values(by='Score', ascending=False))\n",
    "    print('-'*50)\n",
    "\n",
    "for i in range(1, 10):\n",
    "    x_train, y_train, x_test, y_test = train_test_split(X, Y, test_size=i/10, random_state=42)\n",
    "    print('-'*20, i/10)\n",
    "    universal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimenting with different n values in KNN\n",
    "\n",
    "k_range = list(range(1,100))\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X, Y)\n",
    "    y_pred = knn.predict(X)\n",
    "    scores.append(knn.score(X, Y))\n",
    "    \n",
    "plt.plot(k_range, scores)\n",
    "plt.xlabel('Value of k for KNN')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('Accuracy Scores for Values of k of k-Nearest-Neighbors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
